---
title: "Nachrichten Tweets (04.06.2018)"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: "lumen"
    highlight: "tango"
    code_folding: hide
    self_contained: true
---

```{r message=FALSE, warning=FALSE}
library(rtweet)
library(plotly)
library(dplyr)
library(ggplot2)
library(rvest)
library(tidyr)
library(wordcloud2)
library(webshot)
library(htmlwidgets)
library(igraph)
library(stringr)
library(network)

rm(list = ls())
knitr::opts_chunk$set(message=FALSE, warning=FALSE)

# Functions
linebreak <- function(s) {
  gsub('(.{1,70})(\\s|$)', '\\1\n', s)
}

Mycol <- RColorBrewer::brewer.pal(8, "Dark2")
```

## Start 
```{r eval=FALSE, include=FALSE}
rt <- search_tweets("filter:news AND lang:de", 
                    n=15000)

save(rt, file = paste0("../data/", 
                       Sys.Date(),".Rda"))
```

Die letzten 15000 deutschsprachigen Tweets, die einen Link zu einem Nachrichtenartikel beinhalten. Die Tweets wurden über die REST API mit Hilfe des R packetes "rtweet" erhoben. Der gesamte Code ist [hier](https://github.com/franziloew/news_tweets/tree/master/docs) einzusehen. 
Folgende Variablen sind in unserem Datensatz vorhanden. 

```{r}
load("../data/2018-06-04.Rda")

colnames(rt)
```

## Zeitraum
```{r fig.width=10}
ts_plot(rt,"1 minute") +
  theme(plot.title = element_text(face = "bold")) +
  labs(
    x = NULL, y = NULL,
    title = "Nachrichten-Tweets",
    subtitle = "Tweets, die einen Link zu einer Nachrichtenseite beinhalten."
  )
```

### Welche Tweets wurden am häufigsten geteilt?

Die top 20 sind: 
```{r}
rt %>%
  filter(is_retweet == FALSE ) %>%
  dplyr::select(screen_name, text, retweet_count) %>%
  group_by(screen_name, text) %>%
  summarise(retweet_count = sum(retweet_count)) %>%
  arrange(desc(retweet_count)) %>%
  .[1:20,] %>%
  htmlTable::htmlTable(align="l")
  
```

```{r fig.height=10, fig.width=7, message=FALSE, warning=FALSE}
rt %>%
  group_by(screen_name) %>%
  tally(sort = TRUE) -> name_table

name_table %>%
  top_n(50, n) %>%
  ggplot(aes(reorder(screen_name,n),n)) +
  geom_col(fill = Mycol[1], alpha = 0.6) +
  coord_flip() +
  labs(
    x = NULL,
    y = NULL,
    title = "Top 50 Twitter Nutzer...",
    subtitle = "...die einen Nachrichtentweet gesendet haben"
  ) 
```

## Hashtags

```{r fig.height=10, fig.width=7, message=FALSE, warning=FALSE}
rt$hashtags %>%
  unlist() %>%
  na.omit() %>%
  table() %>%
  sort(decreasing = TRUE) %>%
  tibble::as_tibble() -> hash_table

colnames(hash_table) <- c("hashtag", "count")

hash_table %>%
  top_n(50, count) %>%
  ggplot( aes(reorder(hashtag,count), count)) +
  geom_col(fill = Mycol[1], alpha = 0.6) +
  coord_flip() +
  labs(
    x = NULL,
    y = NULL,
    title = "Top 50 Hashtags...",
    subtitle = "...die im Zusammenhang mit einem Nachrichten-Tweet verwendet wurden"
  ) 
```

### Hashtag Wordcloud
```{r}
wordcloud2(hash_table, size = 1)
```

## Nutzer

```{r fig.height=10, fig.width=7, message=FALSE, warning=FALSE}
rt$mentions_screen_name %>%
  unlist() %>%
  na.omit() %>%
  table() %>%
  sort(decreasing = TRUE) %>%
  tibble::as_tibble() -> mentions_table

colnames(mentions_table) <- c("mentions", "count")

mentions_table %>%
  top_n(50, count) %>%
  ggplot( aes(reorder(mentions,count), count)) +
  geom_col(fill = Mycol[1], alpha = 0.6) +
  coord_flip() +
  labs(
    x = NULL,
    y = NULL,
    title = "Top 50 Profil Nennungen...",
    subtitle = "...die im Zusammenhang mit einem Nachrichten-Tweet verwendet wurden"
  ) 
```

### User-Netzwerk

```{r}
rt <- rt %>%
  mutate(poster = str_extract(text,"(RT|via)((?:\\b\\W*@\\w+)+)"),
         who_post = gsub("(RT @|via @)", "", poster, ignore.case=TRUE))

# two column matrix of edges
el <- rt %>%
  filter(is_retweet == TRUE) %>%
  select(screen_name, who_post) %>%
  filter(!grepl("@", who_post)) %>%
  count(who_post, screen_name) %>%
  filter(
    nchar(as.character(who_post))>0 &
      nchar(as.character(screen_name))>0 &
      n > 2
  )
```

```{r fig.height=10, fig.width=10}
rt_graph <- graph_from_data_frame(d=el, directed=T)

glay = layout.fruchterman.reingold(rt_graph)

plot(rt_graph, layout = glay,
     vertex.size = 0.5,
     label.cex = 1,
     vertex.label.family="sans")
title("Retweet Network", cex.main=1, col.main="gray95")
```

```{r eval=FALSE, include=FALSE}
# get urls of each tweet
df <- rt %>%
  mutate(hash_clean1 = NA,
         hash_clean2 = NA,
         url1 = NA,
         url2 = NA,
         mentions1 = NA,
         mentions2 = NA)


for (i in seq(length(df$status_id))) {
  
  df$hash_clean1[i] <- df$hashtags[[i]][1]
  df$hash_clean2[i] <- df$hashtags[[i]][2]
  df$url1[i] <- df$urls_expanded_url[[i]][1]
  df$url2[i] <- df$urls_expanded_url[[i]][2]
  df$mentions1[i] <- df$mentions_screen_name[[i]][1]
  df$mentions2[i] <- df$mentions_screen_name[[i]][2]
}
```

```{r eval=FALSE, include=FALSE}
# scrape title, sitename and description from meta tags of the website url.

# Use 100 most tweeted articles
check_urls <- df %>%
  group_by(url1) %>%
  tally(sort = T) %>%
  .[1:100,]

# these are the meta tags we want to scrape
keeps <- c("og:title","og:site_name","og:description")

for (i in 1:nrow(check_urls)) {
  
  url <- check_urls$url1[i]
  
  meta <- read_html(url) %>% 
    html_nodes("head") %>%
    html_nodes("meta")
  
  temp.df <- bind_rows(lapply(xml_attrs(meta), 
                              function(x) data.frame(as.list(x), stringsAsFactors=FALSE
                                                     )
                              )
                       )
  
  if("property" %in% colnames(temp.df)) {
    
    temp.df <- temp.df %>% 
      filter(property %in% keeps) %>%
      select(content, property) %>%
      mutate(property = gsub("og:","",property)) %>%
      distinct(property, .keep_all = TRUE) %>%
      tidyr::spread(property, content)
  } else {
      temp.df <- as.data.frame(matrix(nrow = 1,ncol = 1))
    }

  
  if("title" %in% colnames(temp.df)) {
    check_urls$title[i] <- temp.df$title
  } else {
    check_urls$title[i] <- NA
  }
  
  if("site_name" %in% colnames(temp.df)) {
    check_urls$site_name[i] <- temp.df$site_name
  } else {
    check_urls$site_name[i] <- NA
  }
  
  if("description" %in% colnames(temp.df)) {
    check_urls$description[i] <- temp.df$description
  } else {
    check_urls$description[i] <- NA
  }
  
  print(i)

}
```

```{r eval=FALSE, include=FALSE}
# combine the article information with the original dataframe
df <- left_join(df, check_urls, by="url1")

save(rt, df, file = paste0("../data/", Sys.Date(),".Rda"))
```

### Welche Artikel wurden am häfigsten geteilt?

```{r}
df %>%
  group_by(title, site_name) %>%
  tally(sort = TRUE) %>%
  htmlTable::htmlTable(align = "l")
```

### Welche Hashtags wurden je News-Website am häufigsten verwendet?

```{r fig.height=16, fig.width=8}
df %>%
  filter(!is.na(title)) %>%
  filter(!is.na(hash_clean1)) %>%
  group_by(site_name, hash_clean1) %>%
  summarise(total = n()) %>%
  arrange(desc(total), .by_group = TRUE) %>%
  slice(1:15) %>%
  ggplot(aes(reorder(hash_clean1,total),total)) +
  geom_col(fill = Mycol[1], alpha=0.6) +
  coord_flip() +
  facet_wrap(~site_name, scales = "free",
             ncol = 2) +
  labs(
    x = NULL,
    y = NULL
  )
```

### Welche Wörter wurden im Zusammenhang mit den Tweets verwendet?

```{r}
df_text <- df %>%
  filter(!is.na(site_name)) %>%
  select(text, site_name)
```

```{r fig.height=14, fig.width=8}
tidy_df <- df_text %>%
  group_by(site_name) %>%
  mutate(
    linenumber = row_number()
  ) %>%
  ungroup() %>%
  tidytext::unnest_tokens(word, text)

# Stopwords entfernen
stopwords <- data_frame(
  word = tm::stopwords("german")
)
custom_sw <- c("https","t.co","via","mal","dass","mehr")

tidy_df <- tidy_df %>%
  anti_join(stopwords, by="word") %>%
  filter(! word %in% custom_sw)

sitenames <- unique(tidy_df$site_name)
```

```{r}
sitenames[1]
```

```{r}
tidy_df %>%
  filter(site_name == sitenames[1]) %>%
  group_by(word) %>%
  summarise(freq = n()) %>%
  arrange(desc(freq)) %>%
  wordcloud2(size=1,
           color = "random-light", 
           backgroundColor = "grey") 
```

```{r}
sitenames[2]
```

```{r}
tidy_df %>%
  filter(site_name == sitenames[2]) %>%
  group_by(word) %>%
  summarise(freq = n()) %>%
  arrange(desc(freq)) %>%
  wordcloud2(size=1,
           color = "random-light", 
           backgroundColor = "grey") 
```

```{r}
sitenames[3]
```

```{r}
tidy_df %>%
  filter(site_name == sitenames[3]) %>%
  group_by(word) %>%
  summarise(freq = n()) %>%
  arrange(desc(freq)) %>%
  wordcloud2(size=1,
           color = "random-light", 
           backgroundColor = "grey") 
```

```{r}
sitenames[5]
```

```{r}
tidy_df %>%
  filter(site_name == sitenames[5]) %>%
  group_by(word) %>%
  summarise(freq = n()) %>%
  arrange(desc(freq)) %>%
  wordcloud2(size=1,
           color = "random-light", 
           backgroundColor = "grey") 
```

```{r}
sitenames[6]
```

```{r}
tidy_df %>%
  filter(site_name == sitenames[6]) %>%
  group_by(word) %>%
  summarise(freq = n()) %>%
  arrange(desc(freq)) %>%
  wordcloud2(size=1,
           color = "random-light", 
           backgroundColor = "grey") 
```

```{r}
sitenames[7]
```

```{r}
tidy_df %>%
  filter(site_name == sitenames[7]) %>%
  group_by(word) %>%
  summarise(freq = n()) %>%
  arrange(desc(freq)) %>%
  wordcloud2(size=1,
           color = "random-light", 
           backgroundColor = "grey") 
```

```{r}
sitenames[8]
```

```{r}
tidy_df %>%
  filter(site_name == sitenames[8]) %>%
  group_by(word) %>%
  summarise(freq = n()) %>%
  arrange(desc(freq)) %>%
  wordcloud2(size=1,
           color = "random-light", 
           backgroundColor = "grey") 
```

```{r}
sitenames[9]
```

```{r}
tidy_df %>%
  filter(site_name == sitenames[9]) %>%
  group_by(word) %>%
  summarise(freq = n()) %>%
  arrange(desc(freq)) %>%
  wordcloud2(size=1,
           color = "random-light", 
           backgroundColor = "grey") 
```

```{r}
sitenames[10]
```

```{r}
tidy_df %>%
  filter(site_name == sitenames[10]) %>%
  group_by(word) %>%
  summarise(freq = n()) %>%
  arrange(desc(freq)) %>%
  wordcloud2(size=1,
           color = "random-light", 
           backgroundColor = "grey") 
```

```{r}
sitenames[13]
```

```{r}
tidy_df %>%
  filter(site_name == sitenames[13]) %>%
  group_by(word) %>%
  summarise(freq = n()) %>%
  arrange(desc(freq)) %>%
  wordcloud2(size=1,
           color = "random-light", 
           backgroundColor = "grey") 
```
