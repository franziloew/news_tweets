---
title: "Nachrichten Tweets (04.06.2018)"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: "lumen"
    highlight: "tango"
    code_folding: hide
    self_contained: true
---

```{r message=FALSE, warning=FALSE}
rm(list = ls())

library(rtweet)
library(dplyr)
library(ggplot2)
library(rvest)
library(tidyr)
library(wordcloud2)
library(igraph)
library(ggraph)
library(stringr)
library(ggraph)
library(tm)
library(tidytext)

## ---- My Functions --- ##

# linebreak for long text
linebreak <- function(s) {
  gsub('(.{1,70})(\\s|$)', '\\1\n', s)
}

# count non -na entries in lists
n <- function(x) {
  unlist(lapply(x, function(y){length(y) - is.na(y[1])}))
}

## --- Set Stylings --- ###
knitr::opts_chunk$set(message=FALSE, warning=FALSE)

theme_set(
  theme_bw(base_size = 14) +
    theme(
      plot.title = element_text(face = "bold", size = 14, 
                                margin = margin(0, 0, 4, 0, "pt")),
      plot.subtitle = element_text(size = 12),
      plot.caption = element_text(size = 6, hjust = 0),
      axis.title = element_text(size = 10),
      panel.border = element_blank()
    )
)

Mycol <- RColorBrewer::brewer.pal(8, "Dark2")
```

## Start 
```{r eval=FALSE, include=FALSE}
rt <- search_tweets("filter:news AND lang:de", 
                    n=18000)

save(rt, file = paste0("../data/", 
                       Sys.Date(),".Rda"))
```

Mit Hilfe des R Packetes "rtweet" wurden über die REST API die 18000 deutschsprachigen Tweets der vergangenen Stunden gesammelt, die einen Link zu einem Nachrichtenartikel beinhalten. Der gesamte Code ist [hier](https://github.com/franziloew/news_tweets/tree/master/docs) einzusehen. 

Folgende Variablen sind in unserem Datensatz vorhanden. 

```{r}
#load("../data/2018-06-04.Rda")

colnames(rt)
```

## Zeitraum
```{r fig.width=8}
ts_plot(rt,"1 minute",
        color = Mycol[3]) +
  theme(plot.title = element_text(face = "bold")) +
  labs(
    x = NULL, y = NULL,
    title = "Nachrichten-Tweets",
    subtitle = "Tweets, die einen Link zu einer Nachrichtenseite beinhalten."
  )
```

### Welche Tweets wurden am häufigsten geteilt?

Die top 20 sind: 
```{r}
rt %>%
  filter(is_retweet == FALSE ) %>%
  dplyr::select(screen_name, text, retweet_count) %>%
  group_by(screen_name, text) %>%
  summarise(retweet_count = sum(retweet_count)) %>%
  arrange(desc(retweet_count)) %>%
  .[1:20,] %>%
  htmlTable::htmlTable(align="l")
```

## Hashtags

```{r fig.height=10, fig.width=7, message=FALSE, warning=FALSE}
rt$hashtags %>%
  unlist() %>%
  na.omit() %>%
  table() %>%
  sort(decreasing = TRUE) %>%
  tibble::as_tibble() -> hash_table

colnames(hash_table) <- c("hashtag", "count")

hash_table %>%
  ggplot( aes(reorder(hashtag,count), count)) +
  geom_col(fill = Mycol[1], alpha = 0.6) +
  coord_flip() +
  labs(
    x = NULL,
    y = NULL,
    title = "Top 50 Hashtags...",
    subtitle = "...die im Zusammenhang mit einem Nachrichten-Tweet verwendet wurden"
  ) 
```

### Hashtag Wordcloud
```{r}
wordcloud2(hash_table, size = 1)
```

```{r}
http <- paste(c("http.*","https.*"), sep = "|")

stopwords <- data_frame(
  word =  stopwords("german")
) %>% rbind(
  data_frame(word = c("t.co","via","mal","dass","mehr", 
                      "schon","rt","gibt"))
)

rt_clean <- rt %>%
  # First, remove http elements manually
  mutate(stripped_text = gsub(http,"", text)) 
  
rt_tidy_words <- rt_clean %>%
  # Second, remove punctuation, convert to lowercase, add id for each tweet!
  dplyr::select(stripped_text) %>%
  unnest_tokens(word, stripped_text) %>%
  
  # Third, remove stop words from your list of words 
  anti_join(stopwords)

# Finally, plot the top 15 words
rt_tidy_words %>%
 count(word, sort = TRUE) %>%
 top_n(20) %>%
 mutate(word = reorder(word, n)) %>%
 ggplot(aes(x = word, y = n, fill = word)) +
 geom_col(fill = Mycol[2],
          alpha = 0.6) +
 xlab(NULL) +
 coord_flip() +
 labs(y = "Count",
 x = "Unique words",
 title = "Most Common words in all News tweets",
 subtitle = '"ufc220" , "Stipe" and "stipemiocicufc" tops our most used words"') + 
  theme(legend.position = "") 
```

## Network Plot
```{r}
# remove punctuation, convert to lowercase, add identity for each tweet!
rt_paired_words <- rt_clean %>%
 dplyr::select(stripped_text) %>%
 unnest_tokens(paired_words, stripped_text, token = "ngrams", n = 2)

# remove stopwords
rt_separated_words <- rt_paired_words %>%
 separate(paired_words, c("word1", "word2"), sep = " ")

rt_filtered <- rt_separated_words %>%
 filter(!word1 %in% stopwords$word) %>%
 filter(!word2 %in% stopwords$word)

# new bigram counts:
rt_words_counts <- rt_filtered %>%
 count(word1, word2, sort = TRUE)
```

```{r fig.height=10, fig.width=10}
# plot word network
rt_words_counts %>%
 filter(n >= 50) %>%
 graph_from_data_frame() %>%
 ggraph(layout = "fr") +
 geom_edge_link(aes(edge_alpha = n, edge_width = n)) +
 geom_node_point(color = "darkslategray4", size = 3) +
 geom_node_text(aes(label = name), vjust = 1.8, size = 7) +
 labs(title = "Word Network: News Tweets",
      subtitle = "Text mining twitter data ",
      x = "", y = "") 
```

## User

```{r fig.height=10, fig.width=7, message=FALSE, warning=FALSE}
rt %>%
  group_by(screen_name) %>%
  tally(sort = TRUE) -> name_table

name_table %>%
  top_n(50, n) %>%
  ggplot(aes(reorder(screen_name,n),n)) +
  geom_col(fill = Mycol[1], alpha = 0.6) +
  coord_flip() +
  labs(
    x = NULL,
    y = NULL,
    title = "Top 50 Twitter Nutzer...",
    subtitle = "...die einen Nachrichtentweet gesendet haben"
  ) 
```

Von diesen Twitter-Usern betrachten wir die Profile der größeren online Nachrichtendienste

```{r}
news <- c("welt", "handelsblatt", "FOCUS_TopNews", 
          "WELTnews", "ntvde", "abendblatt", "FAZ_NET", "Tagesspiegel",
          "SPIEGEL_alles", "wiwo", "zeitonline", "BILD")

name_table %>% 
  filter(screen_name %in% news) %>%
  transmute(screen_name = screen_name,
            n_tweets = n) %>%
  htmlTable::htmlTable(align="l")
```

```{r}
rt_news <- rt_clean %>%

  # create variable indicating wich news profile is mentioned
  mutate(mentions = stri_join_list(stri_extract_all_words(mentions_screen_name),
                                   sep=", ")) %>%
  
  # filter tweets, that mention these profiles
  filter(screen_name %in% news | str_detect(mentions, paste(news, sep="|"))) %>%
  mutate(news_name = ifelse(screen_name %in% news, screen_name, mentions))
```

```{r}
rt_news <- rt_clean %>%
  filter(screen_name %in% news) %>%
  # Second, remove punctuation, convert to lowercase, add id for each tweet!
  dplyr::select(screen_name, stripped_text) %>%
  unnest_tokens(word, stripped_text) %>%
  
  # Third, remove stop words from your list of words 
  anti_join(stopwords) %>%
  
  group_by(screen_name, word) %>%
  count(word)

# Finally, plot the top 15 words
rt_tidy_words %>%
 count(word, sort = TRUE) %>%
 top_n(20) %>%
 mutate(word = reorder(word, n)) %>%
 ggplot(aes(x = word, y = n, fill = word)) +
 geom_col(fill = Mycol[2],
          alpha = 0.6) +
 xlab(NULL) +
 coord_flip() +
 labs(y = "Count",
 x = "Unique words",
 title = "Most Common words in all News tweets",
 subtitle = '"ufc220" , "Stipe" and "stipemiocicufc" tops our most used words"') + 
  theme(legend.position = "") 

for (i in news) {
  rt_news %>%
    filter(screen_name == i) -> temp
    
  wordcloud2()
}
```


